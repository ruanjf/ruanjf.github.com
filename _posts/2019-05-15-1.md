# kubernetes安装
https://www.kubernetes.org.cn/docs
https://www.kubernetes.org.cn/kubernetes%E8%AE%BE%E8%AE%A1%E7%90%86%E5%BF%B5
https://www.kubernetes.org.cn/kubernetes%E8%AE%BE%E8%AE%A1%E6%9E%B6%E6%9E%84
https://v1-11.docs.kubernetes.io/cn/

参看地址
# Window客户端安装
https://blog.frognew.com/2018/03/kubeadm-install-kubernetes-1.10.html
https://storage.googleapis.com/kubernetes-release/release/v1.11.0/bin/windows/amd64/kubectl.exe
# 将文件kubectl.exe拷贝到D:\ProgramFiles\k8s\中，并添加D:\ProgramFiles\k8s\到系统环境变量PATH中，

# minikube安装
https://storage.googleapis.com/minikube/releases/v0.28.2/minikube-windows-amd64.exe
重命名minikube-windows-amd64.exe为minikube.exe
https://yq.aliyun.com/articles/221687



# 服务端安装

# 设置安装代理，由于Google地址需要翻墙
http://welcome66.iteye.com/blog/2175743
echo "proxy=http://172.20.1.45:1080" >> /etc/yum.conf


https://my.oschina.net/xdatk/blog/895645
https://kubernetes.io/docs/setup/independent/install-kubeadm/
https://www.jianshu.com/p/9c7e1c957752
# 配置安装包源
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
exclude=kube*
EOF

# 关闭SELinux
http://roclinux.cn/?p=2264
setenforce 0

# 设置IPtable处理
cat <<EOF > /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system

# 查看k8s版本
yum --showduplicates list |grep kube
# 安装k8s
yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
# 安装特定版本
yum install -y kubelet-1.11.2-0 kubeadm-1.11.2-0 kubectl-1.11.2-0 --disableexcludes=kubernetes
# 保存安装包并离线安装
yum install -y --downloadonly --downloaddir=./packages kubelet kubeadm kubectl --disableexcludes=kubernetes
cd ./packages
yum localinstall -y --nogpgcheck *.rpm --disablerepo=*

# 启动k8s，由于配置未设置启动会失败但不影响，后续会使用kubeadm进行初始化
systemctl enable kubelet && systemctl start kubelet

# 加载systemctl配置
systemctl daemon-reload
systemctl restart kubelet

# 关闭缓存
swapoff -a
# 移除/etc/fstab内的swap


# 添加k8s镜像，由于需要访问外网可能需要翻墙
docker tag 172.20.0.228:5000/k8s/pause:3.1 k8s.gcr.io/pause:3.1

# 添加tag，由于部分组件镜像路径被写死了
docker pull quay.io/coreos/flannel:v0.10.0-amd64
docker tag quay.io/coreos/flannel:v0.10.0-amd64 172.20.0.228:5000/coreos/flannel:v0.10.0-amd64
docker push 172.20.0.228:5000/coreos/flannel:v0.10.0-amd64
docker pull 172.20.0.228:5000/coreos/flannel:v0.10.0-amd64
docker tag 172.20.0.228:5000/coreos/flannel:v0.10.0-amd64 quay.io/coreos/flannel:v0.10.0-amd64

# docker pull kubernets/kubernetes-dashboard-amd64:v1.10.0
# docker tag kubernets/kubernetes-dashboard-amd64:v1.10.0 172.20.0.228:5000/k8s/kubernetes-dashboard-amd64:v1.10.0
# docker push 172.20.0.228:5000/k8s/kubernetes-dashboard-amd64:v1.10.0
docker pull 172.20.0.228:5000/k8s/kubernetes-dashboard-amd64:v1.10.0
docker tag 172.20.0.228:5000/k8s/kubernetes-dashboard-amd64:v1.10.0 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.0

docker pull 172.20.0.228:5000/k8s/metrics-server-amd64:v0.2.1
docker tag 172.20.0.228:5000/k8s/metrics-server-amd64:v0.2.1 k8s.gcr.io/metrics-server-amd64:v0.2.1

# 以下配置在主节点下执行即可，其他工作节点无需操作


# 如果遇到ping ip能通，wget、curl、nc报no route to host需要清理下iptables规则
http://blog.51cto.com/2614223/764757
iptables -F
# 如果多次尝试失败后，需要重置环境
kubeadm reset
systemctl stop kubelet
systemctl stop docker
rm -rf /etc/kubernetes/*
rm -rf /var/lib/cni/
rm -rf /var/lib/kubelet/*
rm -rf /etc/cni/
ifconfig cni0 down
ifconfig flannel.1 down
ifconfig docker0 down
ip link delete cni0
ip link delete flannel.1
systemctl start docker


=================单主节点安装start=========================
# 配置k8s管理镜像，可以使用registry.aliyuncs.com/google_containers镜像
# 开放端口，默认仅开放了30000-32767
# https://github.com/kubernetes/kubeadm/issues/122#issuecomment-389857828
cat <<EOF > kubeadm-config.conf
apiVersion: kubeadm.k8s.io/v1alpha2
kubernetesVersion: v1.11.2
imageRepository: 172.20.0.228:5000/k8s
etcd:
  image: "172.20.0.228:5000/k8s/etcd"
networking:
  podSubnet: 10.244.0.0/16
apiServerExtraArgs:
  service-node-port-range: 1-65535
EOF

# 开发绑定宿主机端口段
https://github.com/kubernetes/kubeadm/issues/122

# 查看k8s组件镜像列表
kubeadm config images list --config=./kubeadm-config.conf

# 拉取k8s组件镜像列表
kubeadm config images pull --config=./kubeadm-config.conf

# 安装单一主节点
# 安装k8s组件
kubeadm init --config=./kubeadm-config.conf
# 安装后复制终端上的kubeadm join，用于加入节点

# 添加k8s用户并加入wheel组
adduser k8s
passwd k8s
usermod -G wheel k8s
su - k8s

# 在k8s用户下拷贝k8s配置到k8s用户中
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
# 可拷贝文件到本地使用，如果是Window环境则可以通过Win + R快捷键输入%userprofile%进入用户目录然后新建文件夹.kube并拷贝admin.conf为config
# 查看配置是否生效
kubectl version

# 安装k8s网络组件
https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml
# 或者先下载后应用
kubectl apply -f kube-flannel.yml

# 如果安装出错，可执行下面命令进行还原k8s组件，执行后可情况kubeadm init操作产生的数据
kubeadm reset
=================单主节点安装end=========================


# 可选，安装高可用版本（3个主节点）
=================高可用版本start=========================
https://v1-11.docs.kubernetes.io/docs/setup/independent/high-availability/
https://kubernetes.io/cn/docs/admin/high-availability/
https://medium.com/@bambash/ha-kubernetes-cluster-via-kubeadm-b2133360b198
https://velotio.com/blog/2018/6/15/kubernetes-high-availability-kubeadm
https://my.oschina.net/u/3133713/blog/1068894#keepalived%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE

# vip     172.20.0.61 （虚拟IP）
# master1 172.20.0.17
# master2 172.20.0.18
# master3 172.20.0.19
# 测试vip是否没在使用
nc -v 172.20.0.61 6443
# 安装keepalived 三台都需要安装
yum install keepalived
systemctl status keepalived
# 配置主节点
mv /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bak
LOAD_BALANCER_DNS=172.20.0.61
KEEPALIVED_STATE=MASTER
KEEPALIVED_INTERFACE=bond0
KEEPALIVED_PRIORITY=100
cat <<EOF > /etc/keepalived/keepalived.conf
! Configuration File for keepalived
global_defs {
  router_id LVS_DEVEL
}

vrrp_script check_apiserver {
  script "/etc/keepalived/check_apiserver.sh"
  interval 3
  weight -2
  fall 10
  rise 2
}

vrrp_instance VI_1 {
    state ${KEEPALIVED_STATE}
    interface ${KEEPALIVED_INTERFACE}
    virtual_router_id 51
    priority ${KEEPALIVED_PRIORITY}
    authentication {
        auth_type PASS
        auth_pass aabbccddeeffgg
    }
    virtual_ipaddress {
        ${LOAD_BALANCER_DNS}
    }
    track_script {
        check_apiserver
    }
}
EOF

# 检测脚本
cat <<EOF > /etc/keepalived/check_apiserver.sh
#!/bin/sh

errorExit() {
    echo "*** \$*" 1>&2
    exit 1
}

curl --silent --max-time 2 --insecure https://localhost:6443/ -o /dev/null || errorExit "Error GET https://localhost:6443/"
if ip addr | grep -q ${LOAD_BALANCER_DNS}; then
    curl --silent --max-time 2 --insecure https://${LOAD_BALANCER_DNS}:6443/ -o /dev/null || errorExit "Error GET https://${LOAD_BALANCER_DNS}:6443/"
fi
EOF

# 重启keepalived，添加开机自启
systemctl restart keepalived
systemctl enable keepalived

# 查看vip绑定信息
ip addr show ${KEEPALIVED_INTERFACE}

# 配置从节点
mv /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bak
LOAD_BALANCER_DNS=172.20.0.61
KEEPALIVED_STATE=BACKUP
KEEPALIVED_INTERFACE=bond0
KEEPALIVED_PRIORITY=99
# 脚本文件如上


# 配置master1
mkdir k8s-m3
cd k8s-m3
LOAD_BALANCER_DNS=172.20.0.61
LOAD_BALANCER_PORT=6443
CP_IP=172.20.0.17
CP_HOSTNAME=sms0004
INITIAL_CLUSTER="${CP_HOSTNAME}=https://${CP_IP}:2380"
cat <<EOF > kubeadm-config-master1.conf
apiVersion: kubeadm.k8s.io/v1alpha2
kind: MasterConfiguration
kubernetesVersion: v1.11.2
imageRepository: 172.20.0.228:5000/k8s
etcd:
  image: "172.20.0.228:5000/k8s/etcd"
apiServerExtraArgs:
  service-node-port-range: 1-65535
apiServerCertSANs:
- "${LOAD_BALANCER_DNS}"
api:
  controlPlaneEndpoint: "${LOAD_BALANCER_DNS}:${LOAD_BALANCER_PORT}"
etcd:
  local:
    extraArgs:
      listen-client-urls: "https://127.0.0.1:2379,https://${CP_IP}:2379"
      advertise-client-urls: "https://${CP_IP}:2379"
      listen-peer-urls: "https://${CP_IP}:2380"
      initial-advertise-peer-urls: "https://${CP_IP}:2380"
      initial-cluster: "${INITIAL_CLUSTER}"
    serverCertSANs:
      - ${CP_HOSTNAME}
      - ${CP_IP}
    peerCertSANs:
      - ${CP_HOSTNAME}
      - ${CP_IP}
networking:
  podSubnet: "10.244.0.0/16"
EOF
# 安装master1
kubeadm init --config=./kubeadm-config-master1.conf

kubeadm join 172.20.0.61:6443 --token trx7pl.bbla9sns0ga2ycf5 --discovery-token-ca-cert-hash sha256:1660e579958c277b0496102269648c89311eba51be0d6cbd4d8bbd619769f0f5

# 查看启动pod，最好在客户端中也进行测试
export KUBECONFIG=/etc/kubernetes/admin.conf
kubectl get pods --all-namespaces

# 安装k8s网络组件
https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml
# 或者先下载后应用
kubectl apply -f kube-flannel.yml


# 拷贝配置
# 可选，设置ssh免密码登录
# 如果未生成ssh key可以使用
ssh-keygen
ssh-copy-id root@172.20.0.18
ssh-copy-id root@172.20.0.19
# 拷贝文件
cat <<EOF > kubeadm-copy-file.sh
USER=root # customizable
CONTROL_PLANE_IPS="172.20.0.18 172.20.0.19"
for host in \${CONTROL_PLANE_IPS}; do
    scp /etc/kubernetes/pki/ca.crt "\${USER}"@\$host:
    scp /etc/kubernetes/pki/ca.key "\${USER}"@\$host:
    scp /etc/kubernetes/pki/sa.key "\${USER}"@\$host:
    scp /etc/kubernetes/pki/sa.pub "\${USER}"@\$host:
    scp /etc/kubernetes/pki/front-proxy-ca.crt "\${USER}"@\$host:
    scp /etc/kubernetes/pki/front-proxy-ca.key "\${USER}"@\$host:
    scp /etc/kubernetes/pki/etcd/ca.crt "\${USER}"@\$host:etcd-ca.crt
    scp /etc/kubernetes/pki/etcd/ca.key "\${USER}"@\$host:etcd-ca.key
    scp /etc/kubernetes/admin.conf "\${USER}"@\$host:
done
EOF
sh kubeadm-copy-file.sh

# 配置master2
mkdir k8s-m3
cd k8s-m3
LOAD_BALANCER_DNS=172.20.0.61
LOAD_BALANCER_PORT=6443
CP_IP=172.20.0.18
CP_HOSTNAME=sms0005
CP0_HOSTNAME=sms0004
INITIAL_CLUSTER="${CP0_HOSTNAME}=https://172.20.0.17:2380,${CP_HOSTNAME}=https://${CP_IP}:2380"
cat <<EOF > kubeadm-config-master2.conf
apiVersion: kubeadm.k8s.io/v1alpha2
kind: MasterConfiguration
kubernetesVersion: v1.11.2
imageRepository: 172.20.0.228:5000/k8s
etcd:
  image: "172.20.0.228:5000/k8s/etcd"
apiServerExtraArgs:
  service-node-port-range: 1-65535
apiServerCertSANs:
- "${LOAD_BALANCER_DNS}"
api:
  controlPlaneEndpoint: "${LOAD_BALANCER_DNS}:${LOAD_BALANCER_PORT}"
etcd:
  local:
    extraArgs:
      listen-client-urls: "https://127.0.0.1:2379,https://${CP_IP}:2379"
      advertise-client-urls: "https://${CP_IP}:2379"
      listen-peer-urls: "https://${CP_IP}:2380"
      initial-advertise-peer-urls: "https://${CP_IP}:2380"
      initial-cluster: "${INITIAL_CLUSTER}"
      initial-cluster-state: existing
    serverCertSANs:
      - ${CP_HOSTNAME}
      - ${CP_IP}
    peerCertSANs:
      - ${CP_HOSTNAME}
      - ${CP_IP}
networking:
  podSubnet: "10.244.0.0/16"
EOF

# 移动配置
cat <<EOF > kubeadm-move-file.sh
mkdir -p /etc/kubernetes/pki/etcd
mv /root/ca.crt /etc/kubernetes/pki/
mv /root/ca.key /etc/kubernetes/pki/
mv /root/sa.pub /etc/kubernetes/pki/
mv /root/sa.key /etc/kubernetes/pki/
mv /root/front-proxy-ca.crt /etc/kubernetes/pki/
mv /root/front-proxy-ca.key /etc/kubernetes/pki/
mv /root/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt
mv /root/etcd-ca.key /etc/kubernetes/pki/etcd/ca.key
mv /root/admin.conf /etc/kubernetes/admin.conf
EOF
sh kubeadm-move-file.sh

# 设置master2
kubeadm alpha phase certs all --config kubeadm-config-master2.conf
kubeadm alpha phase kubelet config write-to-disk --config kubeadm-config-master2.conf
kubeadm alpha phase kubelet write-env-file --config kubeadm-config-master2.conf
kubeadm alpha phase kubeconfig kubelet --config kubeadm-config-master2.conf
systemctl restart kubelet

# 加入etcd集群
export CP0_IP=172.20.0.17
export CP0_HOSTNAME=sms0004
export CP1_IP=172.20.0.18
export CP1_HOSTNAME=sms0005
export KUBECONFIG=/etc/kubernetes/admin.conf
kubectl exec -n kube-system etcd-${CP0_HOSTNAME} -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=https://${CP0_IP}:2379 member add ${CP1_HOSTNAME} https://${CP1_IP}:2380
kubeadm alpha phase etcd local --config kubeadm-config-master2.conf

# 设置master2
kubeadm alpha phase kubeconfig all --config kubeadm-config-master2.conf
kubeadm alpha phase controlplane all --config kubeadm-config-master2.conf
kubeadm alpha phase mark-master --config kubeadm-config-master2.conf

# 查看节点
kubectl get nodes


# 配置master3
mkdir k8s-m3
cd k8s-m3
LOAD_BALANCER_DNS=172.20.0.61
LOAD_BALANCER_PORT=6443
CP_IP=172.20.0.19
CP_HOSTNAME=sms0006
CP0_HOSTNAME=sms0004
CP1_HOSTNAME=sms0005
INITIAL_CLUSTER="${CP0_HOSTNAME}=https://172.20.0.17:2380,${CP1_HOSTNAME}=https://172.20.0.18:2380,${CP_HOSTNAME}=https://${CP_IP}:2380"
cat <<EOF > kubeadm-config-master3.conf
apiVersion: kubeadm.k8s.io/v1alpha2
kind: MasterConfiguration
kubernetesVersion: v1.11.2
imageRepository: 172.20.0.228:5000/k8s
etcd:
  image: "172.20.0.228:5000/k8s/etcd"
apiServerExtraArgs:
  service-node-port-range: 1-65535
apiServerCertSANs:
- "${LOAD_BALANCER_DNS}"
api:
  controlPlaneEndpoint: "${LOAD_BALANCER_DNS}:${LOAD_BALANCER_PORT}"
etcd:
  local:
    extraArgs:
      listen-client-urls: "https://127.0.0.1:2379,https://${CP_IP}:2379"
      advertise-client-urls: "https://${CP_IP}:2379"
      listen-peer-urls: "https://${CP_IP}:2380"
      initial-advertise-peer-urls: "https://${CP_IP}:2380"
      initial-cluster: "${INITIAL_CLUSTER}"
      initial-cluster-state: existing
    serverCertSANs:
      - ${CP_HOSTNAME}
      - ${CP_IP}
    peerCertSANs:
      - ${CP_HOSTNAME}
      - ${CP_IP}
networking:
  podSubnet: "10.244.0.0/16"
kubeletConfiguration:
  baseConfig:
    authentication:
      webhook:
        enabled: true
EOF

# 移动配置
cat <<EOF > kubeadm-move-file.sh
mkdir -p /etc/kubernetes/pki/etcd
mv /root/ca.crt /etc/kubernetes/pki/
mv /root/ca.key /etc/kubernetes/pki/
mv /root/sa.pub /etc/kubernetes/pki/
mv /root/sa.key /etc/kubernetes/pki/
mv /root/front-proxy-ca.crt /etc/kubernetes/pki/
mv /root/front-proxy-ca.key /etc/kubernetes/pki/
mv /root/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt
mv /root/etcd-ca.key /etc/kubernetes/pki/etcd/ca.key
mv /root/admin.conf /etc/kubernetes/admin.conf
EOF
sh kubeadm-move-file.sh

# 设置master3
kubeadm alpha phase certs all --config kubeadm-config-master3.conf
kubeadm alpha phase kubelet config write-to-disk --config kubeadm-config-master3.conf
kubeadm alpha phase kubelet write-env-file --config kubeadm-config-master3.conf
kubeadm alpha phase kubeconfig kubelet --config kubeadm-config-master3.conf
systemctl restart kubelet

# 加入etcd集群
export CP0_IP=172.20.0.17
export CP0_HOSTNAME=sms0004
export CP2_IP=172.20.0.19
export CP2_HOSTNAME=sms0006
export KUBECONFIG=/etc/kubernetes/admin.conf 
kubectl exec -n kube-system etcd-${CP0_HOSTNAME} -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=https://${CP0_IP}:2379 member add ${CP2_HOSTNAME} https://${CP2_IP}:2380
kubeadm alpha phase etcd local --config kubeadm-config-master3.conf

# 设置master3
kubeadm alpha phase kubeconfig all --config kubeadm-config-master3.conf
kubeadm alpha phase controlplane all --config kubeadm-config-master3.conf
kubeadm alpha phase mark-master --config kubeadm-config-master3.conf

# 查看节点
kubectl get nodes
# 查看已安装组件
kubectl get pods --all-namespaces

# 配置主节点可以启动容器
kubectl taint nodes --all node-role.kubernetes.io/master-

# 如果遇到安装问题则可通过以下方式查看日志
# 查看服务状态
systemctl status -l kubelet
# 查看服务器日志
journalctl -exu kubelet
# 查看所有pod状态
kubectl get pods --all-namespaces
# 查看pod日志
kubectl -n <name-space> describe pod <pode-name>
# 查看pod日志示例：查看命名空间kube-system下的pod kubernetes-dashboard日志
kubectl -n kube-system describe pod kubernetes-dashboard


# 配置主节点可以启动容器
kubectl taint nodes --all node-role.kubernetes.io/master-

# 测试高可用是否可执行
ip addr show bond0
systemctl status keepalived
# 使用关闭网络，模拟故障
# ifconfig bond0 down
# 使用docker k8s服务，模拟故障
systemctl stop kubelet
systemctl stop docker
# 产生故障后，参看vip是否漂移，也可通过http访问查看服务是否正常
ip addr show bond0


# 添加k8s用户并加入wheel组
adduser k8s
passwd k8s
usermod -G wheel k8s
su - k8s

# 在k8s用户下拷贝k8s配置到k8s用户中
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
# 可拷贝文件到本地使用，如果是Window环境则可以通过Win + R快捷键输入%userprofile%进入用户目录然后新建文件夹.kube并拷贝admin.conf为config
# 查看配置是否生效
kubectl version

# 查看已安装组件
kubectl get pods --all-namespaces
# 配置主节点可以启动容器
kubectl taint nodes --all node-role.kubernetes.io/master-

# 加入节点，执行kubectl init后将生成类似如下命令用于在节点机器上执行
kubeadm join 172.20.0.228:6443 --token 03d411.xgekdumtdiifs7if --discovery-token-ca-cert-hash sha256:a13ddd1f4276b794c6c97815d7fbbeee965a2ffa5a9f95cc5f06a68355e8336d

# 查看节点列表
kubectl get nodes

===============高可用版本end=============================




# 安装k8s控制台，以下操作不需要在节点中执行
https://www.cnblogs.com/RainingNight/p/deploying-k8s-dashboard-ui.html
https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.0/src/deploy/recommended/kubernetes-dashboard.yaml
kubectl apply -f kubernetes-dashboard.yaml

kubectl -n kube-system patch deployment kubernetes-dashboard -p '
spec:
  template:
    spec:
      containers:
      - name: kubernetes-dashboard
        volumeMounts:
        - name: localtime-file
          mountPath: /etc/localtime
      volumes:
      - name: localtime-file
        hostPath:
          path: /etc/localtime
          type: File
'

# 查看deployment描述
kubectl -n kube-system get deployment kubernetes-dashboard -o yaml

# 查看控制台是否安装成功
kubectl -n kube-system get service kubernetes-dashboard
# 查看详情信息
kubectl -n kube-system describe pod kubernetes-dashboard

# 开启本地访问控制台，启动后控制台将无法使用，通过Ctrl + C可以停止
kubectl proxy
# 访问地址
http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/


# 配置管理用户，用于远程访问控制台
cat <<EOF > admin-user.yaml
---
# admin-user.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kube-system

---
# admin-user-role-binding.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kube-system
EOF

# 创建用户
kubectl create -f admin-user.yaml

# 查看用户Token信息
kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')

# 保存Token
0.61
eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLTZ3djI3Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJkYjNiZDZmNy1jMTNjLTExZTgtOWMzYi0xODY2ZGFhOGNjZWQiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.p64HcT8Kpdmia3o7c7m2u8JIkui6TioNZc-LHZHAASaN5CblojIxTtarJnwVrMf6TnIiB8k5noNT_KLT00cDKCfvdP5PeDlNn0wFHqRAJOe6Xy6N2LMfE6tn6PLBoFCrs0jbDdyM-VomHQZWZJ-5BYFbDXkr0n5efRjUtnX4UFKEcLzoL6pFdPJXl-KGcSR4Jh37ZIKGstZp1pP1mstI82h0TqcZjTF-3TXIXWxg2qDE1NmTDwN-n00217Sq_MUtDqyr4UncFIKynm22Sc0pHzcFcbN8B5OFJ7TJlZWo6eQq7aaZMfCCdrmugQUEZTeKcsV7FEmhUQw3RiKqOgIprA

# 生成证书，用于WEB访问
grep 'client-certificate-data' ~/.kube/config | head -n 1 | awk '{print $2}' | base64 -d > kubecfg.crt
grep 'client-key-data' ~/.kube/config | head -n 1 | awk '{print $2}' | base64 -d > kubecfg.key

# root用户下命令
grep 'client-certificate-data' /etc/kubernetes/admin.conf | head -n 1 | awk '{print $2}' | base64 -d > kubecfg.crt
grep 'client-key-data' /etc/kubernetes/admin.conf | head -n 1 | awk '{print $2}' | base64 -d > kubecfg.key

# 生成p12
openssl pkcs12 -export -clcerts -inkey kubecfg.key -in kubecfg.crt -out kubecfg.p12 -name "kubernetes-client-0.61"

# 访问kubernetes-dashboard
https://172.20.0.61:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/



# 安装NFS文件服务，后期可以替换为glusterfs
http://blog.51cto.com/13109742/2061511
https://www.cnblogs.com/flying607/p/7845146.html
https://www.alibabacloud.com/help/zh/faq-detail/53839.htm
# 查看是否已安装，已安装则可跳过下面一步
yum list installed |grep -E 'nfs|rpcbind'
# 安装NFS服务端
yum install -y nfs-utils rpcbind
# 创建目录
mkdir /opt/nfs-dir
# 设置权限
chmod 777 /opt/nfs-dir
# 配置共享目录
vi /etc/exports
/opt/nfs-dir 172.20.0.0/16(rw,sync,all_squash,anonuid=1000,anongid=1000)
# 启动服务
systemctl start rpcbind
systemctl start nfs
# 设置开机自启
systemctl enable rpcbind
systemctl enable nfs
# 查看共享目录列表，可到其他台机子上执行
showmount -e 172.20.0.19
# 客户端挂载目录
mkdir /opt/k8s-data
mount -t nfs 172.20.0.19:/opt/nfs-dir /opt/k8s-data
# 取消挂载
umount /opt/k8s-data/
# 客户端测试写入10000次总大小80M
time dd if=/dev/zero of=/opt/data/testdd.dbf bs=8k count=10000 conv=fsync
# 客户端测试读取10000次总大小80M
time dd if=/opt/data/testdd.dbf  of=/dev/null bs=8k count=10000 conv=fsync
# 客户端查看nfs挂载信息
nfsstat -m
# 客户端查看nfs信息
nfsstat -c
# 服务端查看nfs信息
nfsstat -s

# 加入开机挂载
vi /etc/fstab
172.20.0.19:/opt/nfs-dir /opt/k8s-data              nfs     defaults        0 0

# 安装基于NFS的持久化存储卷
https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client
https://jimmysong.io/kubernetes-handbook/practice/using-nfs-for-persistent-storage.html

# 本地拉取镜像
docker pull quay.io/external_storage/nfs-client-provisioner:v3.1.0-k8s1.11
docker tag quay.io/external_storage/nfs-client-provisioner:v3.1.0-k8s1.11 172.20.0.228:5000/quay.io/external_storage/nfs-client-provisioner:v3.1.0-k8s1.11
docker push 172.20.0.228:5000/quay.io/external_storage/nfs-client-provisioner:v3.1.0-k8s1.11
# 服务器拉取镜像
docker pull 172.20.0.228:5000/quay.io/external_storage/nfs-client-provisioner:v3.1.0-k8s1.11
docker tag 172.20.0.228:5000/quay.io/external_storage/nfs-client-provisioner:v3.1.0-k8s1.11 quay.io/external_storage/nfs-client-provisioner:v3.1.0-k8s1.11

mkdir nfs-client

# 设置RBAC权限
NS=$(kubectl config get-contexts|grep -e "^\*" |awk '{print $5}')
NAMESPACE=${NS:-default}
cat <<EOF > nfs-client/rbac.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-client-provisioner-runner
rules:
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "update", "patch"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-client-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    namespace: ${NAMESPACE}
roleRef:
  kind: ClusterRole
  name: nfs-client-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: ${NAMESPACE}
roleRef:
  kind: Role
  name: leader-locking-nfs-client-provisioner
  apiGroup: rbac.authorization.k8s.io
EOF
kubectl create -f nfs-client/rbac.yaml

# 部署nfs-client
NFS_SERVER=172.20.0.19
NFS_PATH=/opt/nfs-dir
cat <<EOF > nfs-client/deployment.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-client-provisioner
---
kind: Deployment
apiVersion: extensions/v1beta1
metadata:
  name: nfs-client-provisioner
spec:
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: quay.io/external_storage/nfs-client-provisioner:v3.1.0-k8s1.11
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: fuseim.pri/ifs
            - name: NFS_SERVER
              value: ${NFS_SERVER}
            - name: NFS_PATH
              value: ${NFS_PATH}
      volumes:
        - name: nfs-client-root
          nfs:
            server: ${NFS_SERVER}
            path: ${NFS_PATH}
EOF
kubectl create -f nfs-client/deployment.yaml
# 设置StorageClass
cat <<EOF > nfs-client/class.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: managed-nfs-storage
provisioner: fuseim.pri/ifs # or choose another name, must match deployment's env PROVISIONER_NAME'
parameters:
  archiveOnDelete: "true"
EOF
kubectl create -f nfs-client/class.yaml

# 测试存储
cat <<EOF > nfs-client/test-claim.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: test-claim
  annotations:
    volume.beta.kubernetes.io/storage-class: "managed-nfs-storage"
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Mi
EOF
cat <<EOF > nfs-client/test-pod.yaml
kind: Pod
apiVersion: v1
metadata:
  name: test-pod
spec:
  containers:
  - name: test-pod
    image: quay.io/external_storage/nfs-client-provisioner:v3.1.0-k8s1.11
    command:
      - "/bin/sh"
    args:
      - "-c"
      - "touch /mnt/SUCCESS && exit 0 || exit 1"
    volumeMounts:
      - name: nfs-pvc
        mountPath: "/mnt"
  restartPolicy: "Never"
  volumes:
    - name: nfs-pvc
      persistentVolumeClaim:
        claimName: test-claim
EOF
# 测试创建NFS提供器
kubectl create -f nfs-client/test-claim.yaml -f nfs-client/test-pod.yaml
# 测试删除
kubectl delete -f nfs-client/test-pod.yaml -f nfs-client/test-claim.yaml



# 部署监控
https://github.com/camilb/prometheus-kubernetes
https://github.com/coreos/prometheus-operator/tree/master/contrib/kube-prometheus
https://kairen.github.io/2018/06/23/devops/prometheus-operator/
https://github.com/coreos/prometheus-operator/blob/master/contrib/kube-prometheus/docs/kube-prometheus-on-kubeadm.md

# 下载安装包
wget https://codeload.github.com/camilb/prometheus-kubernetes/zip/4f8f12b08e3e1b0546e7652388b6d93b84b35990
rm -rf prometheus-kubernetes-4f8f12b08e3e1b0546e7652388b6d93b84b35990
unzip prometheus-kubernetes-4f8f12b08e3e1b0546e7652388b6d93b84b35990.zip
cd prometheus-kubernetes-4f8f12b08e3e1b0546e7652388b6d93b84b35990

# 本地拉取镜像，由于部分镜像需要翻墙
cat <<EOF > kube-monitoring-images-tag-push.sh
images="
k8s.gcr.io/kube-state-metrics:v1.3.1
k8s.gcr.io/addon-resizer:1.7
grafana/grafana:5.2.2
quay.io/coreos/configmap-reload:v0.0.1
quay.io/coreos/prometheus-operator:v0.23.1
quay.io/coreos/prometheus-config-reloader:v0.23.0
quay.io/prometheus/alertmanager:v0.15.1
quay.io/prometheus/node-exporter:v0.16.0
quay.io/prometheus/prometheus:v2.3.2
"

for img in \${images}; do
  if [[ \${img} == k8s.gcr.io* ]]
  then
    from=\`echo \${img} | sed 's/k8s\\.gcr\\.io\//anjia0532\/google-containers./g'\`
  else
    from="\${img}"
  fi
  to="172.20.0.228:5000/\${img}"
  docker pull \${from}
  docker tag \${from} \${to}
  docker push \${to}
  echo ""
done
EOF
sh kube-monitoring-images-tag-push.sh

# 服务器上执行
cat <<EOF > kube-monitoring-images-pull.sh
images="
k8s.gcr.io/kube-state-metrics:v1.3.1
k8s.gcr.io/addon-resizer:1.7
grafana/grafana:5.2.2
quay.io/coreos/configmap-reload:v0.0.1
quay.io/coreos/prometheus-operator:v0.23.1
quay.io/coreos/prometheus-config-reloader:v0.23.0
quay.io/prometheus/alertmanager:v0.15.1
quay.io/prometheus/node-exporter:v0.16.0
quay.io/prometheus/prometheus:v2.3.2
"

for img in \${images}; do
  from="172.20.0.228:5000/\${img}"
  to="\${img}"
  docker pull \${from}
  docker tag \${from} \${to}
  echo ""
done
EOF
sh kube-monitoring-images-pull.sh

# 可选，通过打标签的方式指定需要收集监控信息的node，节点名称可以通过kubectl get nodes查看
kubectl label node $NODE_NAME beta.kubernetes.io/monit=prometheus

# 配置数据持久化
https://github.com/coreos/prometheus-operator/blob/master/Documentation/user-guides/storage.md#manual-storage-provisioning
sed -i -e 's/options=("/options=("NFS_Local" "/g' ./deploy
cat <<\EOF > patch.prometheus-k8s.yaml
        "NFS_Local")
            echo "Deploying on custom providers NFS local"
            STORAGE_CLASS_PROVISIONER=fuseim.pri/ifs
            STORAGE_CLASS_TYPE=managed-nfs-storage
            sed -i -e 's,STORAGE_CLASS_PROVISIONER,'"$STORAGE_CLASS_PROVISIONER"',g' manifests/prometheus/prometheus-k8s.yaml;
            sed -i -e 's,STORAGE_CLASS_TYPE,'"$STORAGE_CLASS_TYPE"',g' manifests/prometheus/prometheus-k8s.yaml;
            sed -i -e 's,STORAGE_CLASS_TYPE,'"$STORAGE_CLASS_TYPE"',g' manifests/grafana/grafana.pvc.yaml;
            break
            ;;
EOF
sed -i '78 r patch.prometheus-k8s.yaml' ./deploy

# 配置grafana SMTP告警邮件和告警图片存储
http://docs.grafana.org/installation/configuration/#using-environment-variables
http://blog.itbdw.com/wei-grafana-bao-jing-gong-neng-zeng-jia-jie-tu-gong-neng/
cat <<\EOF > patch.grafana.de.yaml
        env:
        - name: TZ
          value: "Asia/Shanghai"
        - name: GF_SMTP_ENABLED
          value: "true"
        - name: GF_SMTP_HOST
          value: "smtp.qq.com:465"
        - name: GF_SMTP_FROM_ADDRESS
          value: "2881660864@qq.com"
        - name: GF_SMTP_USER
          value: "2881660864@qq.com"
        - name: GF_SMTP_PASSWORD
          value: "agbhebbmkvewdgfb"
        - name: GF_EXTERNAL_IMAGE_STORAGE_PROVIDER
          value: "webdav"
        - name: GF_EXTERNAL_IMAGE_STORAGE_WEBDAV_URL
          value: "https://dav.jianguoyun.com/dav/grafana"
        - name: GF_EXTERNAL_IMAGE_STORAGE_WEBDAV_USERNAME
          value: "runjf@qq.com"
        - name: GF_EXTERNAL_IMAGE_STORAGE_WEBDAV_PASSWORD
          value: "azxymd5pwzguquh3"
EOF
sed -i '22 r patch.grafana.de.yaml' manifests/grafana/grafana.de.yaml
sed -i -e 's/memory: 200Mi/memory: 500Mi/g' manifests/grafana/grafana.de.yaml

# 对外透漏grafana3000端口
sed -i "7a\\  type: NodePort" manifests/grafana/grafana.svc.yaml
sed -i "10a\\    nodePort: 3000" manifests/grafana/grafana.svc.yaml

# 修复新版监控数据地址错误问题
https://github.com/coreos/prometheus-operator/issues/633#issuecomment-367629244
sed -i -e '10,12d' manifests/prometheus/prometheus-k8s-service-monitor-kubelet.yaml
cat <<\EOF > patch.prometheus-k8s-service-monitor-kubelet.yaml
  - port: https-metrics
    scheme: https
    path: /metrics/cadvisor
    interval: 30s
    honorLabels: true
    tlsConfig:
      insecureSkipVerify: true
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
EOF
sed -i '9 r patch.prometheus-k8s-service-monitor-kubelet.yaml' manifests/prometheus/prometheus-k8s-service-monitor-kubelet.yaml

# 执行部署，部署方式选择NFS_Local对应的序号，其他直接按默认值，直接回车即可
./deploy

# 临时访问，测试服务是否启动成功
https://172.20.0.61:6443/api/v1/namespaces/monitoring/services/http:grafana:/proxy/
# 临时访问，本地使用转发端口测试
https://jimmysong.io/kubernetes-handbook/guide/connecting-to-applications-port-forward.html
# 临时访问，Prometheus
kubectl --namespace monitoring port-forward svc/prometheus-k8s 9090
# 临时访问，Grafana
kubectl --namespace monitoring port-forward svc/grafana 3000

# 解决无法获取pod监控数据问题，有问题再执行
https://github.com/coreos/prometheus-operator/blob/master/contrib/kube-prometheus/docs/kube-prometheus-on-kubeadm.md
sed -e "s/- --address=127.0.0.1/- --address=0.0.0.0/" -i /etc/kubernetes/manifests/kube-controller-manager.yaml
sed -e "s/- --address=127.0.0.1/- --address=0.0.0.0/" -i /etc/kubernetes/manifests/kube-scheduler.yaml

# 查看错误信息
kubectl -n monitoring describe pod grafana

# 访问地址，默认用户密码admin/admin
http://172.20.0.61:3000

# 删除监控
./teardown



# 安装日志收集服务
https://github.com/fluent/fluent-bit-kubernetes-logging
https://github.com/fluent/fluent-bit-kubernetes-logging/tree/0f9cd6c1101209ad37d47adae696b1424972129c
https://github.com/fluent/fluent-bit-docs/blob/master/
mkdir fluent-bit
cd fluent-bit
# 下载配置文件
wget https://raw.githubusercontent.com/fluent/fluent-bit-kubernetes-logging/0f9cd6c1101209ad37d47adae696b1424972129c/fluent-bit-service-account.yaml
wget https://raw.githubusercontent.com/fluent/fluent-bit-kubernetes-logging/0f9cd6c1101209ad37d47adae696b1424972129c/fluent-bit-role.yaml
wget https://raw.githubusercontent.com/fluent/fluent-bit-kubernetes-logging/0f9cd6c1101209ad37d47adae696b1424972129c/fluent-bit-role-binding.yaml
wget https://raw.githubusercontent.com/fluent/fluent-bit-kubernetes-logging/0f9cd6c1101209ad37d47adae696b1424972129c/output/elasticsearch/fluent-bit-configmap.yaml
wget https://raw.githubusercontent.com/fluent/fluent-bit-kubernetes-logging/0f9cd6c1101209ad37d47adae696b1424972129c/output/elasticsearch/fluent-bit-ds.yaml
cd ../

# 本地执行，拉取镜像
docker pull fluent/fluent-bit:0.14.3
docker tag fluent/fluent-bit:0.14.3 172.20.0.228:5000/fluent/fluent-bit:0.14.3
docker push 172.20.0.228:5000/fluent/fluent-bit:0.14.3

# 服务器执行，拉取镜像
docker pull 172.20.0.228:5000/fluent/fluent-bit:0.14.3
docker tag 172.20.0.228:5000/fluent/fluent-bit:0.14.3 fluent/fluent-bit:0.14.3
# 镜像拉取使用默认策略，删除imagePullPolicy: Always
sed -i -e '25,25d' fluent-bit/fluent-bit-ds.yaml

# 配置ES地址
FLUENT_ELASTICSEARCH_HOST=172.20.0.56
FLUENT_ELASTICSEARCH_PORT=9200
sed -i -e 's,"elasticsearch","'"$FLUENT_ELASTICSEARCH_HOST"'",g' fluent-bit/fluent-bit-ds.yaml
sed -i -e 's,"9200","'"$FLUENT_ELASTICSEARCH_PORT"'",g' fluent-bit/fluent-bit-ds.yaml

# 设置时区偏移，修复日志记录时间存在时差
https://github.com/rootsongjc/kubernetes-handbook/issues/209#issuecomment-388705822
https://fluentbit.io/documentation/0.14/parser/
sed -i "91a\\        Time_Offset +0800" fluent-bit/fluent-bit-configmap.yaml
sed -i "51a\\        Pipeline        timestamp_pipeline" fluent-bit/fluent-bit-configmap.yaml

# 安装日志收集器
# 创建命名空间
kubectl create namespace logging
# 安装fluent-bit
kubectl apply -f fluent-bit/
# 删除fluent-bit
kubectl delete -f fluent-bit/

# 查看错误信息
kubectl -n logging describe pod fluent-bit



# 可选，安装Helm
https://docs.helm.sh/using_helm/#installing-helm
https://storage.googleapis.com/kubernetes-helm/helm-v2.11.0-linux-amd64.tar.gz
https://storage.googleapis.com/kubernetes-helm/helm-v2.11.0-windows-amd64.zip
拷贝二进制文件到path下
mv linux-amd64/helm /usr/local/bin/helm
# 拉取镜像
docker pull anjia0532/kubernetes-helm.tiller:v2.11.0
docker tag anjia0532/kubernetes-helm.tiller:v2.11.0 172.20.0.228:5000/gcr.io/kubernetes-helm/tiller:v2.11.0
docker push 172.20.0.228:5000/gcr.io/kubernetes-helm/tiller:v2.11.0
# 设置权限
cat <<\EOF > helm-rbac-config.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system
EOF

# 指定镜像地址，并打印安装信息
helm init --service-account tiller --tiller-image=172.20.0.228:5000/gcr.io/kubernetes-helm/tiller:v2.11.0 -o yaml
# 安装，加上--skip-refresh无需下载缓存
helm init --service-account tiller --tiller-image=172.20.0.228:5000/gcr.io/kubernetes-helm/tiller:v2.11.0 --skip-refresh 
# 重置
helm reset



# 部署无线点点项目
mkdir wlyun-api
# 创建命名空间和存储
cat <<EOF > wlyun-api-claim.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: wlyun-api

---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  namespace: wlyun-api
  name: wlyun-nfs-claim
  annotations:
    volume.beta.kubernetes.io/storage-class: "managed-nfs-storage"
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 5000Mi
EOF
kubectl create -f wlyun-api-claim.yaml
# 查看存储
kubectl -n wlyun-api get pv

# 拷贝配置
# 需要先挂载nfs目录/opt/k8s-data
ls /opt/k8s-data | grep wlyun-api-wlyun-nfs-claim-pvc 
cd /opt/k8s-data/wlyun-api-wlyun-nfs-claim-pvc-2c96f388-c25b-11e8-9c3b-1866daa8cced
unzip wlyun-api-config-repo.zip

# 部署项目
kubectl apply -f kubernetes-wlyun-api.yml
kubectl apply -f kubernetes-wlyun-api-local.yml

https://github.com/kubernetes/kubernetes/issues/13488#issuecomment-356892053
# 重启client服务
kubectl -n wlyun-api patch deployment client \
-p '{"spec":{"template":{"spec":{"containers":[{"name":"client","env":[{"name":"RESTART_","value":"'$(date -uIseconds)'"}]}]}}}}'

# 重启mcheck服务
kubectl -n wlyun-api patch deployment mcheck \
-p '{"spec":{"template":{"spec":{"containers":[{"name":"mcheck","env":[{"name":"RESTART_","value":"'$(date -uIseconds)'"}]}]}}}}'

# 重启store服务
kubectl -n wlyun-api patch deployment store \
-p '{"spec":{"template":{"spec":{"containers":[{"name":"store","env":[{"name":"RESTART_","value":"'$(date -uIseconds)'"}]}]}}}}'


kubectl -n wlyun-api patch operation \
-p '{"spec":{"template":{"spec":{"containers":[{"name":"operation","env":[{"name":"RESTART_","value":"'$(date -uIseconds)'"}]}]}}}}'


kubectl -n wlyun-api scale --replicas=2 deployment/store



# 查看mcheck日志
kubectl -n wlyun-api logs -lapp=mcheck --since-time="2018-09-30T14:35:33+08:00"
kubectl -n wlyun-api logs -lapp=mcheck --since=3s
kubectl -n wlyun-api logs -lapp=mcheck --tail=10

kubectl -n wlyun-api logs gateway-7fd6d488c-jrljt --since=10m

kubectl get pods --namespace=kube-system -l k8s-app=kube-dns
kubectl logs --namespace=kube-system coredns-65df74bc46-n5xq6

route add -net 10.244.20.0/24 gw 10.244.20.0 dev flannel.1
route add 10.244.20.0 gw 10.244.20.0 dev flannel.1
route add -net 10.244.20.0/24 10.244.20.0 dev flannel.1

route add 10.244.20.0 mask 255.255.255.0 10.244.20.0 dev flannel.1

route add -net 10.244.20.0/24 flannel.1

route add -net 10.244.20.0 netmask 255.255.255.0 gw 10.244.20.0

ip route add 10.244.20.0/24 via 10.244.20.0 dev flannel.1

